{
  "title": "Simulation Task",
  "description": "The goal of this task is to construct a multi-agent framework that simulates how a user rates (on a scale of 1\u20135 stars) and reviews a given item. The items are drawn from three distinct platforms: the e-commerce site Amazon, the book review platform Goodreads, and the business review site Yelp. The multi-agent system must simulate different users providing ratings and reviews for items originating from these three sources. This task is designed to evaluate the ability of LLM-based agents to generate coherent and contextually appropriate reviews and preference ratings, demonstrating their capacity for user behavior modeling and preference learning. By assessing the effectiveness of LLM agents in simulating human review behaviors, the task contributes to advancing methods in behavioral simulation and offers insights for improving user experience on real-world online review platforms.\n",
  "simulation_focus": "The framework consists of three key agents:\n 1. Planning Agent: Upon receiving a task input (typically a user ID and item ID), this agent is responsible for decomposing the task into executable steps. For example, it may first identify the user\u2019s profile information and then retrieve the item\u2019s attributes.\n 2. Memory Agent: This agent maintains all task-relevant historical information, including item details linked to the item ID, past reviews of the item, the user\u2019s profile, and the user\u2019s review history.\n 3. Reasoning Agent: Using information retrieved from memory (user and item data) and the plan provided by the planning agent, this agent performs reasoning with an LLM to simulate the user\u2019s behavior. It outputs a predicted star rating and review text as the final simulated result.",
  "data_folder": "data_fitting/agent_society/",
  "data_files": {
    "amazon_train_sample.json": "The files contain records from Amazon platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only\u2014they must not be provided to the model to avoid data leakage.",
    "goodreads_train_sample.json": "The files contain records from Goodreads platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only\u2014they must not be provided to the model to avoid data leakage.",
    "yelp_train_sample.json": "The files contain records from Yelp platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only\u2014they must not be provided to the model to avoid data leakage.",
    "user_sample.json": "Indexed by \"user_id\", this file contains detailed information about each user.",
    "item_sample.json": "Indexed by \"item_id\", this file provides metadata about each item.",
    "review_sample.json": "Indexed by \"user_id\" and \"item_id\", this file includes historical reviews written by users about specific items.",
    "keys.py": "This file contains the keys for the data files. Please use 'OPENAI_API_KEY' from it to call openai api."
  },
  "evaluation_metrics": {
    "Preference Estimation": {
      "description": "The travel distance between each consecutive decision step within a trajectory is collected. This metric evaluates the spatial pattern of an individual\u2019s activities by measuring the distance between two consecutive locations in a trajectory.",
      "metric": "1\u2212Mean\u00a0Absolute\u00a0Error\u00a0(MAE) of predicted star ratings, indicating deviation from actual user preferences."
    },
    "Review Generation": {
      "description": "The review generation is calculated based on the review metrics.",
      "metric": "1 - (Emotional Tone Error * 0.25 + Sentiment Attitude Error * 0.25 + Topic Relevance Error * 0.5), indicating the deviation from actual reviews."
    },
    "Overall Quality": {
      "description": "The overall quality is calculated based on the preference estimation and review generation.",
      "metric": "Metric: (Preference Estimation + Review Generation) / 2, indicating the overall quality of the simulated reviews."
    }
  },
  "simulation_type": "agent_based",
  "entities": [
    {
      "name": "User",
      "attributes": [
        "id",
        "preferences",
        "review_history",
        "rating_tendency"
      ],
      "behaviors": [
        "write_review",
        "rate_product"
      ]
    },
    {
      "name": "Product",
      "attributes": [
        "id",
        "category",
        "average_rating",
        "review_count"
      ],
      "behaviors": [
        "receive_review",
        "receive_rating"
      ]
    }
  ],
  "interactions": [
    {
      "name": "User reviews product",
      "description": "A user writes a review for a product and gives it a star rating.",
      "entities_involved": [
        "User",
        "Product"
      ]
    }
  ],
  "parameters": {
    "number_of_users": 1000,
    "number_of_products": 50,
    "review_probability": 0.1,
    "rating_scale": 5
  },
  "metrics": [
    {
      "name": "average_rating",
      "description": "Average rating of the product based on user reviews."
    },
    {
      "name": "review_count",
      "description": "Total number of reviews received by the product."
    }
  ],
  "validation_criteria": [
    {
      "name": "rating_distribution",
      "description": "Distribution of ratings should match real-world data."
    }
  ],
  "prediction_period": null
}