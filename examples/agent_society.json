{
  "task_objective": {
    "description": "The goal of this task is to construct a multi-agent framework that simulates how a user rates (on a scale of 1–5 stars) and reviews a given item. The items are drawn from three distinct platforms: the e-commerce site Amazon, the book review platform Goodreads, and the business review site Yelp. The multi-agent system must simulate different users providing ratings and reviews for items originating from these three sources. This task is designed to evaluate the ability of LLM-based agents to generate coherent and contextually appropriate reviews and preference ratings, demonstrating their capacity for user behavior modeling and preference learning. By assessing the effectiveness of LLM agents in simulating human review behaviors, the task contributes to advancing methods in behavioral simulation and offers insights for improving user experience on real-world online review platforms.\n",
    "simulation_focus": "The framework consists of three key agents:\n 1. Planning Agent: Upon receiving a task input (typically a user ID and item ID), this agent is responsible for decomposing the task into executable steps. For example, it may first identify the user’s profile information and then retrieve the item’s attributes.\n 2. Memory Agent: This agent maintains all task-relevant historical information, including item details linked to the item ID, past reviews of the item, the user’s profile, and the user’s review history.\n 3. Reasoning Agent: Using information retrieved from memory (user and item data) and the plan provided by the planning agent, this agent performs reasoning with an LLM to simulate the user’s behavior. It outputs a predicted star rating and review text as the final simulated result.",
    "Data Calibration Procedure": "For validation, 30 records are randomly sampled from each of the three data sources (Amazon, Goodreads, Yelp). These are used to validate the multi-agent framework and calibrate agent behavior accordingly.",
    "Simulator Design Requirements": "The simulator must include the three aforementioned agents (planning, memory, reasoning). Additionally, all data files should be indexed to allow the memory agent to efficiently retrieve relevant entries—such as item metadata, user profiles, and review histories—based on the user ID and item ID of the input task.",
    "Input-Output Mapping of the Simulated Agent Framework": "Input: A (user_id, item_id) pair. User ID: Identifies the user whose behavior is to be simulated. Item ID: Represents the product, service, or content for which a review is to be generated.\n  Output (Simulated Results): A structured dictionary containing:\n  Star Rating: A numerical value indicating the simulated user’s overall opinion.\n  Review Text: A contextually relevant review informed by the user’s preferences and the item’s characteristics."
  },
  "data_folder": "data_fitting/agent_society/",
  "data_files": {
    "amazon_train_sample.json": "The files contain records from Amazon platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only—they must not be provided to the model to avoid data leakage.",
    "goodreads_train_sample.json": "The files contain records from Goodreads platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only—they must not be provided to the model to avoid data leakage.",
    "yelp_train_sample.json": "The files contain records from Yelp platforms. The fields \"user_id\" and \"item_id\" should be used as task inputs. The fields \"stars\" and \"review\" represent the ground-truth outputs and are for validation only—they must not be provided to the model to avoid data leakage.",
    "user_sample.json": "Indexed by \"user_id\", this file contains detailed information about each user.",
    "item_sample.json": "Indexed by \"item_id\", this file provides metadata about each item.",
    "review_sample.json": "Indexed by \"user_id\" and \"item_id\", this file includes historical reviews written by users about specific items."
  },
  "evaluation_metrics": {
    "Preference Estimation": {
      "description": "The travel distance between each consecutive decision step within a trajectory is collected. This metric evaluates the spatial pattern of an individual’s activities by measuring the distance between two consecutive locations in a trajectory.",
      "metric": "1−Mean Absolute Error (MAE) of predicted star ratings, indicating deviation from actual user preferences."
    },
    "Review Generation": {
      "description": "The review generation is calculated based on the review metrics.",
      "metric": "1 - (Emotional Tone Error * 0.25 + Sentiment Attitude Error * 0.25 + Topic Relevance Error * 0.5), indicating the deviation from actual reviews."
    },
    "Overall Quality": {
      "description": "The overall quality is calculated based on the preference estimation and review generation.",
      "metric": "Metric: (Preference Estimation + Review Generation) / 2, indicating the overall quality of the simulated reviews."
    }
  }
}
